1: началось всё с того, решил поставить два пода оператора
2: попробовал отключить ноду сразу с подом оператора и с инстансом постгреса:
в конечном итоге оказалось всё завязанным на операторе, который рулил тем
кто будет следующим мастером, приёмники на реплике отключились автоматически
это радует. После того, как kubelet обнаружил, что все поды порушились,
то он восстановил под оператора на другой ноде и потом нормально мастера
поменял, дальше реплика поднялась, вроде, нормально.
3: потом поставил два инстанса оператора и он в начале работал не совсем
как надо, но после перезагрузки он начал переключаться между операторами
через выборы лидера, не очень хочется отдавать это на kubernetes, но,
похоже, подругому это не организуешь
начались приколы: когда я останавливаю ноду с оператором основным и
с мастером постгреса, то есть оператор меняет под и потом выполняет
переключение мастера. Однако когда поднимается нода со старым мастером,
то он не может нормально считать wal через pg_rewind
TODO: почему-то ещё генерируется куча логов, которые видеть так-то не хочется
но когда не происходит переключение, то объём wal остаётся таким же
4: почистил кластер и выполнил переключение такое же, но оператор находился
на другой ноде, а убивал мастера. Он снова упал и конфиги испортил
5: вычистил оператор, оставил его одного, и снова убил ноду с мастером.
подождал пока в консоли отобразится информация о смерти подов на ноде,
включил ноду ииии... всё заработало, он выполнил pg_rewind накатил, значит
логи, объёмом чуть меньше 100мб и потом нормально присоедилился к мастеру.
6: провёл отключение ещё раз и не дождался появления ответа о отключении подов
и всё полетело с ошибкой соединения к первичному инстансу debug-non-wait.json
7: ещё раз отключил ту же ноду и он уже выдавал ошибку о неправильной
конфигурации и отсутствии команды восстановления.
8: патрони тоже генерирует много логов, уже почти полгигабайта навалил
9: после очистки хранилища всё прекрасно восстанавливается, на
объём хранилища грешить не получается, так как у патрони давно всё
должно было забиться
10: запустим убийство мастера на другой ноде и дождёмся ответа kubelet
убьём его вместе с одним оператором, поставлю ещё другой
11: тоже упало всё debug-two-new-oper-failover.json, при чём второй раз
с ошибкой о аварийном завершении подключения к серверу
это уже что-то интересное, а может быть и нет. Почему у патрони нет
такой проблемы?
12: а потом начинаются приколы с убитой конфигурацией, почему, не понятно
она по какой-то причине не обновляется
13: прочистить всё снова, выполнить сначала отключение пода главного
и снять всё подключения, которые идут к реплике и когда они завершаются
провести ту же операцию, но только с падением ноды и выжиданием долгим
А потом без выжидания тоже проследить
14: под восстанавливается адекватно и там не меняется ip, восстановление
происходит достаточно быстро
15: на падении ноды было выяснено, что инстанс получает конфигурацию
посредством запроса к первичному инстансу, скорее всего для
согласованноти параметров, патрони же так не делает, он забирает конфиги
прямо из секрета, поэтому она и не теряется
А второй раз запуск вообще не возможен, так как конфиги помирают и
не могут быть восстановлены
16: подождём пока все поды получат статус умерших
17: следующий раз запустим сразу
поды помечаются умершими через 10 минут
18: видимо там вся папка пытается копироваться
и где-то оно ломается нафиг
19: сервис просто берёт и перестаёт отвечать на посланные данные пакет №2867
именно в этот момент он зависает на целых три секунды!!! три секунды, карл,
потом он очухивается и просит приостановить передачу, ещё почему-то в этом соединении
нет согласованноти по ширине окна на той и друой сторонах
В итоге они друг другу посылают кучу пакетов с флагами rst и потом соединение
накрывается
ещё интересный момент с опросом статуса, там у нас база данных ещё не стартует
20: чтобы сломать под - надо послать сигнал kill -19 pid и убить под - это
не даёт серверу нормально завершить работу и потом при восстановлении он начинает
копировать данные, а уже на копировании данных он помирает
21: теперь необходимо провести всю эту процедуру сначала, но с нодой, у которой больше данных,
когда она становится репликой, то как себя поведёт
Возможно, ещё эта проблема вызвана тем, что я не включаю слоты репликации
и поэтому pg_rewind не может понять что необходимо копировать, а что нет
    однако я больше думаю на резкое завершение работы, которое приводит к поломке в процессе
    pg_rewind
хах, всё оказалось не так просто, после убийства пода он не обновляет в начале процесса
конфиг и поэтому не теряется возможность запуска сервера
pg_rewind падает потом сервер перезапускается и снова поднимается pg_rewind
потом он снова падает. Падает не на чётко определённом месте, а в рандомном
от то загрузит логи до 1го таймлайна, то до второго, а то вообще до пятого
возникает ощущение что такое поведение связано с самой сетью или с
невозможностью для pg_rewind нормально обработать тот поток данных, который передаётся
от одного к другому
22: то ли ему повезло, то ли ещё чего, но под с большим количеством логов смог откоприровать
логи с пода, у которого их было меньше.
Вот и думай, то ли ему повезло просто с количеством логов, то ли это pg_rewind
потом резко вспоминает что ему это всё не надо копировать.
23: Пакеты от первичного к реплике идут с ip сервиса кубернетес, он туда как
бы и устанавливает соединение, однако в какой-то момент на интерфейс cbr0
приходит ответ к тому же порту клинета но с ip самого пода,
я думал это проблема tcp.spurious_retransmission, но они раньше тоже
посылались, при чём с ip сервиса
#############TODO: получается, в какой-то момент на клинет ответы начинают приходить
с другого ip, клиенту это не нравится и он пытается закрыть это соединение
попутно грохая то соединение, которое нормальное
Возможно это связано с тем, что в начале сервис хочет в балансировку
и поэтому сохраняется его ip, но потом что-то происходит и это перестаёт
работать.
А может это связано с перезагрузкой кластера, когда обновляется информация о сервисе?
типа в ip tables стирается запись о перезаписи ip и он проходит без этой операции
В patrony в качестве имени сервиса откуда выполнять копирование используется прям имя
пода, который в настоящий момент является лидером, похоже это и позволяет избежать
проблем при копировании большого массива данных
ВЫВОД: надо или использовать patrony или заставить cloudnative использовать
имя пода в качестве адреса, а не сервис, вроде бы после обновления кэша kubernetes
у меня будет актуальное имя первичного сервера.
А можно ли это как-то сделать?
Или лучше поискать информацию о проблемах сервисов?
Или это всё-таки из-за одновления правил iptables
Если смотреть по логам, то там нет обновлений кластера, надо было снять
ещё логи самого оператора
24: оператор никак не модифицирует сервисы кубернетеса, он просто помечает
метками соответствующие поды, как первичные или реплики. Это всё, конечно
круто, но это не позволит потом обращаться напрямую к мастеру или к
реплике, а нужно будет использовать сервисы. А ну как оператор решит
выполнить пометку подов во время потоковой загрузки? Что тогда-то?
